{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strange-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-savage",
   "metadata": {},
   "source": [
    "## Electronic Books\n",
    "Accessing the project gutenberg books. **This method did not work as expected because of numerous hidden characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "modern-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "numeric-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "thick-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = str(urlopen(url).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "royal-victim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "major-reaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1358498"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "voluntary-behavior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'\\\\xef\\\\xbb\\\\xbfThe Project Gutenberg EBook of Crime and Punishment, by Fyodo\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-billion",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Produces the familial structure, a listof words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "handed-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eastern-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225597"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "furnished-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'\\\\xef\\\\xbb\\\\xbfThe\",\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky\\\\r\\\\n\\\\r\\\\nThis',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "southern-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "orange-satin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hindu-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'which',\n",
       " 'made',\n",
       " 'him',\n",
       " 'scowl',\n",
       " 'and',\n",
       " 'feel',\n",
       " 'ashamed',\n",
       " '.',\n",
       " 'He',\n",
       " 'was\\\\r\\\\nhopelessly',\n",
       " 'in',\n",
       " 'debt',\n",
       " 'to',\n",
       " 'his',\n",
       " 'landlady',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'meeting',\n",
       " 'her.\\\\r\\\\n\\\\r\\\\nThis',\n",
       " 'was',\n",
       " 'not',\n",
       " 'because',\n",
       " 'he',\n",
       " 'was',\n",
       " 'cowardly',\n",
       " 'and',\n",
       " 'abject',\n",
       " ',',\n",
       " 'quite',\n",
       " 'the',\n",
       " 'contrary',\n",
       " ';',\n",
       " 'but\\\\r\\\\nfor',\n",
       " 'some',\n",
       " 'time',\n",
       " 'past']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1020:1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "impossible-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Katerina Ivanovna',\n",
       " 'Pyotr Petrovitch',\n",
       " 'Avdotya Romanovna',\n",
       " 'Pulcheria Alexandrovna',\n",
       " 'Rodion Romanovitch',\n",
       " 'Marfa Petrovna',\n",
       " '\\\\xe2\\\\x80\\\\x9d cried',\n",
       " 'Sofya Semyonovna',\n",
       " '\\\\xe2\\\\x80\\\\x9d said',\n",
       " 'old woman',\n",
       " '\\\\xe2\\\\x80\\\\x9d Raskolnikov',\n",
       " 'Porfiry Petrovitch',\n",
       " 'Project Gutenberg-tm',\n",
       " 'don\\\\xe2\\\\x80\\\\x99t know',\n",
       " 'great deal',\n",
       " 'Amalia Ivanovna',\n",
       " 'young man',\n",
       " 'Hay Market',\n",
       " 'police station',\n",
       " 'Nikodim Fomitch']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-extent",
   "metadata": {},
   "source": [
    "Manually finding the positions to crop the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "phantom-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5866"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "indoor-costa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1338290"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.rfind(\"End of Project\") #reverse find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "quiet-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw[5866:1338290]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "finnish-snake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-lying",
   "metadata": {},
   "source": [
    "## Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portable-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "described-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naval-reproduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a sample text file to practice loading a file into a nltk corpus.  lets see how this works, but I wonder if it will be ok.  Let's give it a go and maybe we will get lucky.\\n\\nI like to use multiple lines with some whitespace in between because that is the kind of annoying stuff that I have to deal with.\\n\\nOh boy this new DOTA show is fetishly and poorly paced.  Tazerface!!\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loose-virgin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter_3_Processing_Raw_Text.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'cfd_methods.png',\n",
       " 'Chapter_1_language_processing.ipynb',\n",
       " 'brown_corpus.png',\n",
       " 'test.txt',\n",
       " 'Chapter_2_Corpora_and_lexical_relations.ipynb',\n",
       " 'nltk_freq_func.png',\n",
       " 'basic_corpus_methods.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just an axample of using os\n",
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text file to practice loading a file into a nltk corpus.  lets see how this works, but I wonder if it will be ok.  Let's give it a go and maybe we will get lucky.\n",
      "\n",
      "I like to use multiple lines with some whitespace in between because that is the kind of annoying stuff that I have to deal with.\n",
      "\n",
      "Oh boy this new DOTA show is fetishly and poorly paced.  Tazerface!!\n"
     ]
    }
   ],
   "source": [
    "# Reading one line at a time (strip removes \"\\n\")\n",
    "f.seek(0)\n",
    "for line in f:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-cross",
   "metadata": {},
   "source": [
    "## The NLP Pipeline\n",
    "\n",
    "![](NLP_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-crystal",
   "metadata": {},
   "source": [
    "Checking the types along the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entire-stanford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = open('test.txt','r').read()\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescription-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "impaired-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w.lower() for w in tokens]\n",
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "varying-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(words))\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-traveler",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "regulated-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-bernard",
   "metadata": {},
   "source": [
    "**Using Basic Metacharacters**\n",
    "\n",
    "Using re.search we find words that end in 'ed' using the expression 'ed$' which matches the end of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cultural-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('ed$', w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-suicide",
   "metadata": {},
   "source": [
    "The '.' symbol is a **wildcard** that matches and single character. This finds eight letter words where the third letter is j and the sixth letter is t. The \"^\" chracter matches the beginning of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "descending-arena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-niger",
   "metadata": {},
   "source": [
    "The '?' metacharacter specifies that the previous character is optional so that \"^e-?mail$\" will match \"email\" and \"e-mail\". Additionally, sum can be used to count the number of occurances of a given pattern instead of returning a list of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "supported-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"\n",
    "        This is a little sentence about e-mail. \n",
    "        You can spell e-mail as e-mail or email but I guess\n",
    "        I prefer e-mail, but in no case do we refer to e-mail\n",
    "        as electronic mail because that makes us sound like \n",
    "        we are 100 years old.\n",
    "      \"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "words = [w.lower() for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stunning-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for w in words if re.search('^e-?mail$', w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-photography",
   "metadata": {},
   "source": [
    "**Ranges and Closures**\n",
    "\n",
    "***Example*** The T9 system for texting makes it so that two words could be created using the same numbers example hole and golf both are entered with the sequence (4653) and are known as **textonyms**.\n",
    "\n",
    "![](T9.png)\n",
    "\n",
    "So to find all if the textonyms with golf we can use the regular expression \"^[ghi][mno][jlk][def]$\" where the brackets are closures indicating that any letter within the backet can match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "placed-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search(\"^[ghi][mno][jlk][def]$\",w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-chick",
   "metadata": {},
   "source": [
    "Using '-' with in the brackets specifies a range and a + represents repetitions. i.e. one or more instances of the preceeding item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "enhanced-playback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gloom',\n",
       " 'glooming',\n",
       " 'gnomon',\n",
       " 'go',\n",
       " 'gog',\n",
       " 'gogo',\n",
       " 'goi',\n",
       " 'going',\n",
       " 'gol',\n",
       " 'goli']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is using numbers 4, 5, and 6\n",
    "[w for w in wordlist if re.search('^[g-o]+$',w)][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "forced-cheese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'abloom',\n",
       " 'abode',\n",
       " 'abolla',\n",
       " 'aboma',\n",
       " 'aboon',\n",
       " 'academe',\n",
       " 'acana',\n",
       " 'acca',\n",
       " 'accede']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is using numbers 2, 3, 5, and 6\n",
    "[w for w in wordlist if re.search('^[a-fj-o]+$',w)][20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-restaurant",
   "metadata": {},
   "source": [
    "More exploration into using \"+\" which is helpful on the chat corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "composite-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "smaller-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m+i+n+e+$',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "incident-offering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahahah',\n",
       " 'ahh',\n",
       " 'ahhahahaha',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhh',\n",
       " 'ahhhhhhhhhhhhhh',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'haaa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahaaa',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaa',\n",
       " 'hahahah',\n",
       " 'hahahaha',\n",
       " 'hahahahaaa',\n",
       " 'hahahahahaha',\n",
       " 'hahahahahahaha',\n",
       " 'hahahahahahahahahahahahahahahaha',\n",
       " 'hahahhahah',\n",
       " 'hahhahahaha']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^[ha]+$',w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-recommendation",
   "metadata": {},
   "source": [
    "The \"\\*\" operator returns everything that the '+' operator does but also instances where the preceeding term does not exists.\n",
    "\n",
    "The \"^\" operator has a different meaning inside of brackets and indicates exclution of the characters inside the brackets. So \"^[aeiouAEIOU]+$ would match things that had no values.\n",
    "\n",
    "Other symbols include \\\\, \\{\\}, (), and | which are given in examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "greater-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "broke-confusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \\ is the escape key for certain characters\n",
    "[w for w in wsj if re.search('[0-9]+\\.[0-9]+$', w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "normal-enemy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another \\ example\n",
    "[w for w in wsj if re.search(\"^[A-Z]+\\$$\",w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "grateful-present",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Curly bracket indicated that the preceeding item should be repeated\n",
    "# exactly the specified number of times.\n",
    "[w for w in wsj if re.search('^[0-9]{4}$',w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "unknown-glenn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here a range is given in the brackets so the words will \n",
    "# be between 3 and 5 letters.\n",
    "[w for w in wsj if re.search(\"^[0-9]+-[a-z]{3,5}$\",w)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "committed-lighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here a minimum is set for the first set {5,} and a maximum was set\n",
    "# fo the last set {,6}\n",
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "better-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | is an \"or\" operator meaning the pattern can be matched by\n",
    "# one of the patterns on either side. () indicate the scope of\n",
    "# | operator\n",
    "[w for w in wsj if re.search('(ed|ing)$',w)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-civilization",
   "metadata": {},
   "source": [
    "## Useful Applications of RegEx\n",
    "\n",
    "**Extracting Word Pieces**\n",
    "\n",
    "Using re.findall() finds all non-overlapping matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "administrative-denmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all vowels and count them\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "focused-lancaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re.findall(r'[aeiou]', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "introductory-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('ea', 476), ('oi', 65), ('ou', 329), ('io', 549), ('ee', 217), ('ie', 331), ('ui', 95), ('ua', 109), ('ai', 261), ('ue', 105), ('ia', 253), ('ei', 86), ('iai', 1), ('oo', 174), ('au', 106), ('eau', 10), ('oa', 59), ('oei', 1), ('oe', 15), ('eo', 39), ('uu', 1), ('eu', 18), ('iu', 14), ('aii', 1), ('aiia', 1), ('ae', 11), ('aa', 3), ('oui', 6), ('ieu', 3), ('ao', 6), ('iou', 27), ('uee', 4), ('eou', 5), ('aia', 1), ('uie', 3), ('iao', 1), ('eei', 2), ('uo', 8), ('uou', 5), ('eea', 1), ('ueui', 1), ('ioa', 1), ('ooi', 1)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All sequences of two or more vowels in some text and\n",
    "# their relative frequency.\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "                      for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "freelance-aviation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2009, 12, 31]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int(n) for n in re.findall(r'[0-9]{2,}', '2009-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-tuesday",
   "metadata": {},
   "source": [
    "The following example retains initial and end vowels but removes all internal vowels to show how english text represented in this way is still readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ongoing-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "color-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "early-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "protective-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-missile",
   "metadata": {},
   "source": [
    "Combining regular expressions with conditional frequency distributions. Here we extract all consonant-vowel sequences and create a conditional frequency distribution. This is for a dictionary of the language rotokas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fewer-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sensitive-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "passing-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "certain-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-world",
   "metadata": {},
   "source": [
    "Inspecting the words behind the numbers. The following returns the words that contain a specific consonant vowel combination. Method is to create a dictionary-esque index of consonant-vowel combinations and their associated words with nltk.Index()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "focal-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'kasuari' => ('ka','kasuari'), ('su','kasuari'),('ri','kasuari'),\n",
    "cv_word_pairs = [(cv, w) for w in rotokas_words\n",
    "                         for cv in re.findall(r'[ptksvr][aeiou]',w)]\n",
    "\n",
    "# Turns the above into {'ka':['kasuari','ka...',...],'su':['kasuari'],...}\n",
    "cv_index = nltk.Index(cv_word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "residential-albania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_index['su']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ready-traveler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaapo',\n",
       " 'kaapopato',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapo']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_index['po'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-patient",
   "metadata": {},
   "source": [
    "**Finding Word Stems**\n",
    "\n",
    "Methods to strip the suffixes from words and only consider the stems. NLTK has built-in stemmers but here are some ways to do this with and without regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sustainable-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "testa\n",
      "ted\n",
      "exceeding\n",
      "exceed\n"
     ]
    }
   ],
   "source": [
    "# without regex (not very good)\n",
    "def stem(word):\n",
    "    for suffix in ['ing','ly','ed','ious','ies','ive','es','s','ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "example = ['test','testing','tested','tests','testament','tedious',\n",
    "          'exceedingly','exceeds']\n",
    "for word in example:\n",
    "    print(stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "enhanced-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with regex first build up a disjunction of all the suffixes\n",
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$','processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "forty-cursor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because of the parenthesis, which limits the scope of\n",
    "# the disjunction, only the matched suffix 'ing' is returned.\n",
    "# To use parenthesis to limit the scope of the disjunction bu\n",
    "# not the output '?:' must be added with in the parenthesis.\n",
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$','processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fifth-individual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'd actually live to split the word into stem and suffix\n",
    "# so we will parenthesize botth parts\n",
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$','processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "nasty-spencer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But this does not work well in this case...\n",
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$','processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "preceding-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...this is because the '\\*' operator is greedy so adding\n",
    "# the non-greedy operator '?' will give the desired output\n",
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$','processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "literary-cigarette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even works with an empty suffix if the second statement is \n",
    "# optional as well\n",
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$','language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "indie-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('proces', 's')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But there are still issues...\n",
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$','process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "square-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "pediatric-accessory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"\n",
    "        DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "        is no basis for a system of government. Supreme executive power derives from\n",
    "        a mandate from the masses, not from some farcical aquatic ceremony.\n",
    "      \"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-seating",
   "metadata": {},
   "source": [
    "## Searching Tokenized Text\n",
    "\n",
    "A special kind of regular expression is used for searching across multiple words in a text (list of tokens). For example, \"\\<a\\> \\<man\\>\" finds all instances of \"a man\" in the text.  \\<\\> are used to mark token boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "public-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "contrary-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "modular-stress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "# Since the parenthesis indicate the scope of the match,\n",
    "# any word that occurs between 'a'and 'man'are returned\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "tribal-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = nltk.Text(nps_chat.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "headed-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dramatic-living",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-senator",
   "metadata": {},
   "source": [
    "Discovering **hypernyms** using regular expressions and searching for language of  the form \"x and other ys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "accepting-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "polyphonic-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies','learned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "preliminary-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-dialogue",
   "metadata": {},
   "source": [
    "## Normalizing Text\n",
    "E.g. using lower() to ignore the differences between words and their capitalized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "floating-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        DENNIS: Listen, strange women lying in ponds distributing swords\\n        is no basis for a system of government. Supreme executive power derives from\\n        a mandate from the masses, not from some farcical aquatic ceremony.\\n      '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "identical-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-empty",
   "metadata": {},
   "source": [
    "## Stemmers\n",
    "These built in stemming functions are more sophisticated than the custom built ones explored earlier. The Porter stemmer can even handles complocated situations such as the word lying (mapping it to lie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "minus-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "vertical-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "interim-circus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-richards",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "WordNetLemmatizer removes affixes only if the resulting word is in its dictionary.  Good for compiling vocabulary that results in a list of valid lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "wrapped-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "stuffed-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-jimmy",
   "metadata": {},
   "source": [
    "## Regex for Tokenizing Text\n",
    "Most simple is splitting on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "psychological-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"\n",
    "        'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "        though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "        well without--Maybe it's always pepper that makes people hot-tempered,'...\n",
    "      \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "academic-blank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " \"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\\n\",\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r' ',raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eight-treasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\",\n",
       " '']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding in newlines and tabs\n",
    "re.split(r'[ \\t\\n]+',raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "optimum-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\",\n",
       " '']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the \\s operator can simplify this and matches any whitespace\n",
    "# \\S is the complement of this and matches non-whitespace characters\n",
    "re.split(r'\\s+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "constitutional-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addtionally we can use \\w to indicate words and \\W to \n",
    "# indicate the opposite i.e andything other than letters,\n",
    "# digits, or underscores\n",
    "re.split(r'\\W+',raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dirty-worry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " 'I',\n",
       " \"'M\",\n",
       " 'a',\n",
       " 'Duchess',\n",
       " ',',\n",
       " \"'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " ',',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " ')',\n",
       " ',',\n",
       " \"'I\",\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " '-',\n",
       " '-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " '-tempered',\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this conditional first tries to split whitespace and then non-\n",
    "# whitespace character followed by whitespace\n",
    "re.findall(r'\\w+|\\S\\w*', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "legitimate-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "# generalize the \\w+ to permit word-internal hyphens and apostrophes\n",
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-elements",
   "metadata": {},
   "source": [
    "![](regex1.png)\n",
    "![](regex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-teddy",
   "metadata": {},
   "source": [
    "## NLTK's RegEx Tokenizer\n",
    "similar to re.findall() but more efficient. Does not need special treatment for parenthesis.\n",
    "\n",
    "***NOTE:*** the special (?x) \"verbose flag\" tells python to strip out the embedded whitespace and comments when the regular expression is annotated and written over several lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "changed-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dated-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I DON'T KNOW WHY THIS ISN'T WORKING...\n",
    "pattern = r'''(?x)     # set flag to allow verbose regex\n",
    "      ([A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(-\\w+)*       # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40,82%\n",
    "    | \\.\\.\\.           # ellipsis\n",
    "    | [][.,;\"'?():-_`] # these are separate tokens\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "demanding-anxiety",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', ''),\n",
       " ('A.', '', ''),\n",
       " ('', '-print', ''),\n",
       " ('', '', ''),\n",
       " ('', '', '.40'),\n",
       " ('', '', '')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-postcard",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "In some cases the text is only available as a stream of characters. before tokenizing the text into words we need to segment itinto sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "floppy-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "micro-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "broad-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "focused-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the wild events which were to follow this girl had no\\npart at all; he never saw her again until all his tale was over.', 'And yet, in some indescribable way, she kept recurring like a\\nmotive in music through all his mad adventures afterwards, and the\\nglory of her strange hair ran like a red thread through those dark\\nand ill-drawn tapestries of the night.', 'For what followed was so\\nimprobable, that it might well have been a dream.', 'When Syme went out into the starlit street, he found it for the\\nmoment empty.', 'Then he realised (in some odd way) that the silence\\nwas rather a living silence than a dead one.', 'Directly outside the\\ndoor stood a street lamp, whose gleam gilded the leaves of the tree\\nthat bent out over the fence behind him.', 'About a foot from the\\nlamp-post stood a figure almost as rigid and motionless as the\\nlamp-post itself.', 'The tall hat and long frock coat were black; the\\nface, in an abrupt shadow, was almost as dark.', 'Only a fringe of\\nfiery hair against the light, and also something aggressive in the\\nattitude, proclaimed that it was the poet Gregory.', 'He had something\\nof the look of a masked bravo waiting sword in hand for his foe.']\n"
     ]
    }
   ],
   "source": [
    "print(sents[171:181])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-sewing",
   "metadata": {},
   "source": [
    "Sometimes it is necessary to find the word breaks.  The segmentation below the text are boolean values indicating possible word and sentence breaks. There n-1 characters in the segmentation string since that is the maximum number of locations possible for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "altered-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "freelance-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cross-ceramic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "incoming-jewel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-gospel",
   "metadata": {},
   "source": [
    "An objective function can be defined to assign a score whose value will be optimized based on the sie of a lexicon and the amount of information neede to reconstruct the source text from the lexicon.\n",
    "\n",
    "![](ObjFunc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "center-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = len(' '.join(list(set(words))))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "facial-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "applied-fields",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'like',\n",
       " 'thedogg',\n",
       " 'y']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "distinct-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ambient-object",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ordinary-tyler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-truck",
   "metadata": {},
   "source": [
    "With a function that evaluates a score, the goal is to maximize this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "motivated-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "medium-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "regional-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0,len(segs)-1))\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "earned-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs ,evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, int(round(temperature)))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "effective-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "opposed-gabriel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "59 ['doyou', 's', 'eethekitty', 's', 'e', 'ethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "59 ['doyou', 's', 'eethekitty', 's', 'e', 'ethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "48 ['doyou', 's', 'eethe', 'ki', 'tty', 's', 'eethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "48 ['doyou', 's', 'eethe', 'ki', 'tty', 's', 'eethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "48 ['doyou', 's', 'eethe', 'ki', 'tty', 's', 'eethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "48 ['doyou', 's', 'eethe', 'ki', 'tty', 's', 'eethe', 'doggy', 'doyou', 'likethe', 'ki', 'tty', 'likethe', 'doggy']\n",
      "45 ['doyou', 's', 'eethe', 'kitty', 's', 'eethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "42 ['doyou', 'seethe', 'kitty', 'seethe', 'doggy', 'doyou', 'likethe', 'kitty', 'likethe', 'doggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100000100001000001000010000100000010000100000010000'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-tourism",
   "metadata": {},
   "source": [
    "With enough data it would be possible to segment words automatically with some degree of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-fundamental",
   "metadata": {},
   "source": [
    "**String Formatting Expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "appreciated-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "floral-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog->4;\n",
      "cat->3;\n",
      "snake->1;\n"
     ]
    }
   ],
   "source": [
    "for word in fdist:\n",
    "    print('%s->%d;' % (word, fdist[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "disciplinary-effectiveness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->3;'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The characters %s and %d are place holder for strings and digits\n",
    "'%s->%d;' % ('cat', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "pressing-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want a coffee right now'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I want a %s right now\" % 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "promotional-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'Lee wants a %s right now'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "atomic-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = ['sandwich','spam fritter','pancake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "heated-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "for snack in menu:\n",
    "    print(template % snack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-rates",
   "metadata": {},
   "source": [
    "**Lining Things Up**\n",
    "\n",
    "It is possible to use these specifiers to set specific sizes for the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "alert-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   dog'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adds the space before\n",
    "'%6s' % 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "adjusted-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog   '"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adds the space after\n",
    "'%-6s' % 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "lightweight-fiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog   '"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the width parametrically\n",
    "width = 6\n",
    "'%-*s' % (width,'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "literary-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.1867%'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f is used to specify rounding for floating point numbers\n",
    "count, total = 3205, 9375\n",
    "\"accuracy for %d words: %2.4f%%\" % (total, 100*count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-cornwall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
